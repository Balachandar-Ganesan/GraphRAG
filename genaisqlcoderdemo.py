# -*- coding: utf-8 -*-
"""GenAISQLCoderDemo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XrzIGv8E-GhPdh-rTlC9syNzDkCa2S1W
"""

!pip install torch transformers bitsandbytes accelerate sqlparse

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "defog/sqlcoder-7b-2"
tokenizer = AutoTokenizer.from_pretrained(model_name)

    # if you have atleast 15GB of GPU memory, run load the model in float16
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",
        use_cache=True,
    )

prompt = """### Task
Generate a SQL query to answer [QUESTION]{question}[/QUESTION]

### Instructions
- If you cannot answer the question with the available database schema, return 'Check EspnCricInfo'
- Remember that battingaverage is sum of Runs divided by number of matches
- Remember that Century is Runs greater than or equal to 100


### Database Schema
This query will run on a database whose schema is represented in this string:
CREATE TABLE Scores (
  MatchID INTEGER PRIMARY KEY, -- Unique ID for each Match
  Opposition VARCHAR(50), -- Name of cricket team
  Innings INTEGER, -- batted first or second
  Runs INTEGER  -- Runs Scored in the match
);


"""

import sqlparse

def generate_query(question):
    updated_prompt = prompt.format(question=question)
    inputs = tokenizer(updated_prompt, return_tensors="pt").to("cuda")
    generated_ids = model.generate(
        **inputs,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens=400,
        do_sample=False,
        num_beams=1,
    )
    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    # empty cache so that you do generate more results w/o memory crashing
    # particularly important on Colab â€“ memory management is much more straightforward
    # when running on an inference service
    return sqlparse.format(outputs[0].split("[SQL]")[-1], reindent=True)

question = "Total Runs Scored against each opposition"
generated_sql = generate_query(question)
print(generated_sql)

question = "Runs scored during First Innings"
generated_sql = generate_query(question)
print(generated_sql)

question = "BattingAverage"
generated_sql = generate_query(question)
print(generated_sql)

question = "List of Matches where scores was century"
generated_sql = generate_query(question)
print(generated_sql)